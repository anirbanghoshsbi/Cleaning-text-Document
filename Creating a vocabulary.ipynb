{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load the document in memory\n",
    "def load_doc(filename):\n",
    "    with open (filename , 'r') as file:\n",
    "        text = file.read()\n",
    "        return text\n",
    "# tur  a doc into clean token\n",
    "def clean_doc(doc):\n",
    "    tokens=doc.split()\n",
    "    #preprocess regex for char filtering\n",
    "    re_punc=re.compile('[%s]' %re.escape(string.punctuation))\n",
    "    # remove puntuation from word\n",
    "    tokens = [re.sub(re_punc , ' ' , word) for word in tokens]\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in  tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words =set(stopwords.words('english'))\n",
    "    # remove the stop words\n",
    "    tokens = [ word for word in tokens if not word in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "def add_doc_to_vocab(filename , vocab):\n",
    "    #load doc\n",
    "    doc= load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "    \n",
    "#load all documents in a directory\n",
    "def process_docs(directory , vocab):\n",
    "                       #walk though all files in  the folder\n",
    "                       for filename in listdir(directory):\n",
    "                           #skip test review\n",
    "                           if filename.startswith('cv9'):\n",
    "                               continue\n",
    "                           #create a directory path\n",
    "                           path = directory + '/' + filename\n",
    "                           add_doc_to_vocab(path , vocab)\n",
    "#save file to list\n",
    "def save_file(lines , filename):\n",
    "                       data = '\\n'.join(lines)\n",
    "                       with open(filename , 'w') as file:\n",
    "                           file.write(data)\n",
    "                       \n",
    "#save file to list bigram model\n",
    "\n",
    "def save_file_bigram(lines , filename):\n",
    "    data= '\\n'.join(' '.join(line) for line in bigrams)\n",
    "    with open(filename ,'w') as file:\n",
    "                file.write(data)\n",
    "def save_file_trigram(lines , filename):\n",
    "    data= '\\n'.join(' '.join(line) for line in trigrams)\n",
    "    with open(filename , 'w') as file:\n",
    "                file.write(data)\n",
    "                file.write('\\n')\n",
    "                       \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36053\n",
      "[('film', 7974), ('one', 4939), ('movie', 4815), ('like', 3193), ('even', 2261), ('good', 2073), ('time', 2039), ('story', 1899), ('would', 1843), ('much', 1823), ('also', 1757), ('get', 1723), ('character', 1699), ('two', 1642), ('characters', 1618), ('first', 1586), ('see', 1553), ('way', 1515), ('well', 1477), ('make', 1418), ('really', 1400), ('little', 1347), ('films', 1338), ('life', 1329), ('plot', 1286), ('people', 1267), ('could', 1248), ('bad', 1246), ('scene', 1240), ('never', 1197), ('best', 1176), ('new', 1139), ('scenes', 1132), ('many', 1129), ('man', 1122), ('know', 1092), ('movies', 1027), ('great', 1011), ('another', 992), ('action', 980), ('love', 975), ('us', 967), ('go', 950), ('director', 947), ('something', 944), ('end', 943), ('still', 935), ('seems', 930), ('back', 921), ('made', 911)]\n"
     ]
    }
   ],
   "source": [
    "#define a vocab\n",
    "from nltk.util import ngrams\n",
    "vocab = Counter()\n",
    "#add all docs to vocab\n",
    "process_docs('txt_sentoken/pos' ,vocab)\n",
    "process_docs('txt_sentoken/neg' , vocab)\n",
    "#print the length of the vocab\n",
    "print(len(vocab))\n",
    "#keep min occurance to 2\n",
    "min_occurance =2\n",
    "tokens=[k for k,c in vocab.items() if c>min_occurance]\n",
    "print(vocab.most_common(50))\n",
    "save_file(tokens , 'vocab.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty']\n",
      "[('films', 'adapted'), ('adapted', 'comic'), ('comic', 'books')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(Counter(ngrams(vocab , 2)))\n",
    "#save the vocab\n",
    "print(tokens[:5])\n",
    "# Create a bigram BOW\n",
    "#print(bigrams.most_common(2))\n",
    "print(bigrams[:3])\n",
    "save_file_bigram(bigrams , 'vocab_bigram.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('films', 'adapted', 'comic'), 1), (('adapted', 'comic', 'books'), 1)]\n"
     ]
    }
   ],
   "source": [
    "trigrams =Counter(ngrams(vocab ,3))\n",
    "print(trigrams.most_common(2))\n",
    "save_file_trigram(trigrams , 'vocab_trigram.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = '\\n'.join(' '.join(line) for line in trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Output.txt\", \"w\") as text_file:\n",
    "    text_file.write(trigrams)\n",
    "    text_file.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating bi--grams\n",
    "\n",
    "#bigrams = [b for l in text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "#print(bigrams)\n",
    "#print('\\n'.join(''.join(elems) for elems in bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .join expects a sequence of strings, but you're giving it a sequence of tuples.\n",
    "\n",
    "To get the result you posted, you'll need to join each element in each tuple, and then join each tuple together:\n",
    "\n",
    "print('\\n'.join(''.join(elems) for elems in data))\n",
    "This works because .join will accept a generator expression, allowing you to iterate over data (your list of tuples).\n",
    "\n",
    "We therefore have two joins going on: the inner join builds a string of the three letters (eg, 'ABC'), and the outer join places newline characters ('\\n') between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
